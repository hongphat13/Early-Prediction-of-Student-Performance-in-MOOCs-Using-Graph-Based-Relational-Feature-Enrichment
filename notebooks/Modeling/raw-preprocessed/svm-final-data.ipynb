{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":237833078,"sourceType":"kernelVersion"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:39:35.517488Z","iopub.execute_input":"2025-05-05T16:39:35.517743Z","iopub.status.idle":"2025-05-05T16:39:38.775405Z","shell.execute_reply.started":"2025-05-05T16:39:35.517718Z","shell.execute_reply":"2025-05-05T16:39:38.774872Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Đánh giá dữ liệu","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import (\n    f1_score, precision_score, recall_score, accuracy_score,\n    roc_auc_score, roc_curve, confusion_matrix, classification_report\n)\nimport matplotlib.pyplot as plt\nimport seaborn as plt\nimport seaborn as sns\nimport numpy as np\n\ndef evaluate_model(model, X_train, y_train, X_test, y_test):\n    def evaluate_split(X, y, split_name):\n        y_pred = model.predict(X)\n        y_proba = model.predict_proba(X)\n\n        f1 = f1_score(y, y_pred, average=None)\n        precision = precision_score(y, y_pred, average=None)\n        recall = recall_score(y, y_pred, average=None)\n        accuracy = accuracy_score(y, y_pred)\n\n        y_bin = label_binarize(y, classes=[0, 1, 2, 3, 4])\n        roc_auc = roc_auc_score(y_bin, y_proba, multi_class=\"ovr\")\n\n        print(f\"\\n--- {split_name} Metrics ---\")\n        print(\"F1 Score (per class):\", f1)\n        print(\"Precision (per class):\", precision)\n        print(\"Recall (per class):\", recall)\n        print(f\"Accuracy: {accuracy:.4f}\")\n        print(f\"AUC (One-vs-Rest): {roc_auc:.4f}\")\n\n        print(\"\\nClassification Report:\")\n        print(classification_report(y, y_pred, target_names=[\"E\", \"D\", \"C\", \"B\", \"A\"]))\n\n        conf_matrix = confusion_matrix(y, y_pred)\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n                    xticklabels=[\"E\", \"D\", \"C\", \"B\", \"A\"],\n                    yticklabels=[\"E\", \"D\", \"C\", \"B\", \"A\"])\n        plt.title(f\"Confusion Matrix - {split_name}\")\n        plt.xlabel(\"Predicted Label\")\n        plt.ylabel(\"True Label\")\n        plt.show()\n\n        # ROC Curve\n        plt.figure(figsize=(10, 8))\n        for i in range(5):\n            fpr, tpr, _ = roc_curve(y_bin[:, i], y_proba[:, i])\n            auc_score = roc_auc_score(y_bin[:, i], y_proba[:, i])\n            plt.plot(fpr, tpr, label=f\"Class {i} (AUC = {auc_score:.2f})\")\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.title(f\"ROC Curve - {split_name}\")\n        plt.xlabel(\"False Positive Rate\")\n        plt.ylabel(\"True Positive Rate\")\n        plt.legend()\n        plt.show()\n\n        return f1, precision, recall, accuracy, roc_auc\n\n    train_metrics = evaluate_split(X_train, y_train, \"Train\")\n    test_metrics = evaluate_split(X_test, y_test, \"Test\")\n\n    return {\n        \"train\": train_metrics,\n        \"test\": test_metrics\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:39:38.777163Z","iopub.execute_input":"2025-05-05T16:39:38.777455Z","iopub.status.idle":"2025-05-05T16:39:38.786951Z","shell.execute_reply.started":"2025-05-05T16:39:38.777437Z","shell.execute_reply":"2025-05-05T16:39:38.786224Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Permutation Importance","metadata":{}},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef plot_permutation_importance(model, X_train, X_train_scaled, y_train):\n    # Tính Permutation Importance\n    perm_importance = permutation_importance(model,X_train_scaled, y_train, n_repeats=8, random_state=42)\n    \n    # Tạo DataFrame cho dữ liệu quan trọng\n    importance_df = pd.DataFrame({\n        'Feature': X_train.columns,\n        'Importance': perm_importance.importances_mean\n    }).sort_values(by='Importance', ascending=False)\n    \n    # Dựa trên số lượng feature, điều chỉnh kích thước của biểu đồ\n    num_features = len(importance_df)\n    figsize = (10, num_features * 0.3)  # Điều chỉnh chiều cao dựa trên số feature\n    \n    # Vẽ biểu đồa\n    importance_df.plot(kind='barh', x='Feature', y='Importance', legend=False, figsize=figsize)\n    plt.title(\"Permutation Importance\")\n    plt.xlabel(\"Importance\")\n    plt.ylabel(\"Feature\")\n    plt.show()\n\n    return importance_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:39:38.787815Z","iopub.execute_input":"2025-05-05T16:39:38.788007Z","iopub.status.idle":"2025-05-05T16:39:39.025777Z","shell.execute_reply.started":"2025-05-05T16:39:38.787992Z","shell.execute_reply":"2025-05-05T16:39:39.025043Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Lasso","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.linear_model import LassoCV\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef lasso_feature_selection(X_train, X_train_scaled, y_train):\n    # Lasso Regression với Cross-Validation để tìm alpha tốt nhất\n    lasso = LassoCV(cv=5, random_state=42)\n    lasso.fit(X_train_scaled, y_train)\n    \n    # In ra trọng số của các feature\n    feature_importance = pd.DataFrame({\n        'Feature': X_train.columns,\n        'Importance': np.abs(lasso.coef_)\n    }).sort_values(by='Importance', ascending=False)\n    \n    # Tính chiều cao dựa trên số lượng feature\n    num_features = len(feature_importance)\n    height = max(4, num_features * 0.3)  # đảm bảo không quá nhỏ\n    \n    # Vẽ biểu đồ trọng số của feature\n    feature_importance.plot(kind='barh', x='Feature', y='Importance', legend=False, figsize=(8, height))\n    plt.title(\"Lasso Feature Importance\")\n    plt.xlabel(\"Importance\")\n    plt.ylabel(\"Feature\")\n    plt.tight_layout()\n    plt.show()\n    \n    return feature_importance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:39:39.026570Z","iopub.execute_input":"2025-05-05T16:39:39.026791Z","iopub.status.idle":"2025-05-05T16:39:39.032309Z","shell.execute_reply.started":"2025-05-05T16:39:39.026775Z","shell.execute_reply":"2025-05-05T16:39:39.031510Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Boruta Trick","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef boruta_trick(model, X_train, X_train_scaled, y_train):\n    # Tạo một feature ngẫu nhiên (feature \"bóng\") và scale nó tương tự\n    random_feature = np.random.rand(X_train.shape[0], 1)\n\n    # Tính min và max của X_train_scaled (giả sử scale về [0, 1] hoặc tương tự)\n    min_val = np.min(X_train_scaled)\n    max_val = np.max(X_train_scaled)\n\n    # Scale random feature về cùng khoảng\n    scaled_random_feature = min_val + random_feature * (max_val - min_val)\n\n    X_train_random_scaled = np.hstack([X_train_scaled, scaled_random_feature])\n    feature_names = list(X_train.columns) + ['Random Feature']\n\n    # Huấn luyện mô hình trên dữ liệu đã scale có thêm feature ngẫu nhiên\n    model.fit(X_train_random_scaled, y_train)\n\n    # Tính toán importance của từng feature\n    feature_importance = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': model.feature_importances_\n    }).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n\n    # In ra độ quan trọng của tất cả các feature (bao gồm feature ngẫu nhiên)\n    print('Feature Importance (Including Random Feature):')\n    print(feature_importance)\n\n    # Xác định ngưỡng quan trọng là độ quan trọng của feature ngẫu nhiên\n    random_feature_importance = feature_importance[feature_importance['Feature'] == 'Random Feature']['Importance'].iloc[0]\n\n    # Lọc các feature quan trọng hơn feature ngẫu nhiên\n    important_features = feature_importance[feature_importance['Importance'] > random_feature_importance]\n    print('\\nImportant features (Above Random Feature):')\n    print(important_features)\n\n    # Dựa trên số lượng feature, điều chỉnh kích thước của biểu đồ\n    num_features = len(feature_importance)\n    figsize = (10, max(6, num_features * 0.4))  # Điều chỉnh chiều cao dựa trên số feature\n                                                 # Đảm bảo chiều cao tối thiểu là 6\n\n    # Vẽ biểu đồ\n    plt.figure(figsize=figsize)\n    plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n    plt.title(\"Boruta Feature Importance (Trick) - Scaled Data\")\n    plt.xlabel(\"Importance\")\n    plt.ylabel(\"Feature\")\n    plt.tight_layout()\n    plt.show()\n\n    return important_features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:39:39.033116Z","iopub.execute_input":"2025-05-05T16:39:39.033320Z","iopub.status.idle":"2025-05-05T16:39:39.050476Z","shell.execute_reply.started":"2025-05-05T16:39:39.033306Z","shell.execute_reply":"2025-05-05T16:39:39.049935Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Calibration","metadata":{}},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\n\ndef calibrate_model(model, X_train, y_train, method='sigmoid'):\n    \"\"\"\n    Áp dụng Calibration (One-vs-Rest cho đa lớp).\n    \n    Parameters:\n    model: mô hình đã huấn luyện\n    X_train: Dữ liệu huấn luyện\n    y_train: Nhãn huấn luyện\n    method: phương pháp calibration ('sigmoid' hoặc 'isotonic')\n    \n    Return:\n    calibrated_model: mô hình đã qua calibration\n    \"\"\"\n    calibrated_model = CalibratedClassifierCV(model, method=method, cv='prefit')\n    calibrated_model.fit(X_train, y_train)\n    return calibrated_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:39:39.051193Z","iopub.execute_input":"2025-05-05T16:39:39.051436Z","iopub.status.idle":"2025-05-05T16:39:39.077422Z","shell.execute_reply.started":"2025-05-05T16:39:39.051415Z","shell.execute_reply":"2025-05-05T16:39:39.076832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SHAP (SHapley Additive exPlanations)\nimport shap\n\ndef plot_shap_importance(model, X_train, X_train_scaled):\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X_train_scaled)\n    # Vẽ SHAP summary plot\n    shap.summary_plot(shap_values, X_train)\n    return shap_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:39:39.079531Z","iopub.execute_input":"2025-05-05T16:39:39.080120Z","iopub.status.idle":"2025-05-05T16:39:44.785480Z","shell.execute_reply.started":"2025-05-05T16:39:39.080103Z","shell.execute_reply":"2025-05-05T16:39:44.784923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shap\nimport pandas as pd\nimport numpy as np\n\ndef plot_shap_importance_svm(model, X_train, X_train_scaled):\n    \"\"\"\n    Tính toán và vẽ SHAP importance cho mô hình SVM bằng KernelExplainer.\n\n    Args:\n        model: Mô hình SVM đã được huấn luyện.\n        X_train (pd.DataFrame): DataFrame chứa các đặc trưng huấn luyện (chưa scale).\n        X_train_scaled (np.ndarray): Mảng NumPy chứa các đặc trưng huấn luyện đã scale.\n\n    Returns:\n        np.ndarray: Mảng chứa các giá trị SHAP.\n    \"\"\"\n    # Sử dụng KernelExplainer cho các mô hình không dựa trên cây\n    explainer = shap.KernelExplainer(model.predict_proba, X_train_scaled)\n    shap_values = explainer.shap_values(X_train_scaled)\n\n    # Vẽ SHAP summary plot\n    # Lưu ý: KernelExplainer trả về một list các mảng SHAP values cho từng lớp\n    # Chúng ta có thể chọn một lớp cụ thể để hiển thị hoặc lấy trung bình\n    if model.probability:\n        shap.summary_plot(shap_values, X_train)\n    else:\n        print(\"Mô hình SVM cần được huấn luyện với probability=True để sử dụng predict_proba cho SHAP.\")\n\n    return shap_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:39:44.786180Z","iopub.execute_input":"2025-05-05T16:39:44.786491Z","iopub.status.idle":"2025-05-05T16:39:44.791433Z","shell.execute_reply.started":"2025-05-05T16:39:44.786475Z","shell.execute_reply":"2025-05-05T16:39:44.790640Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_evaluate_svm_per_phase_v2(phases):\n    \"\"\"\n    Huấn luyện và đánh giá mô hình SVM cho từng phase dữ liệu.\n    Phiên bản này loại bỏ các cột 'total_score', 'label', 'label_encoded'\n    khỏi features trước khi xử lý.\n\n    Args:\n        phases (list): Một danh sách các tuple, mỗi tuple chứa (train_df, test_df)\n                       cho một phase cụ thể.\n    \"\"\"\n    all_results = {}\n\n    for i, (train_df, test_df) in enumerate(phases):\n        phase_name = f\"Phase {i+1}\"\n        print(f\"\\n--- Processing {phase_name} ---\")\n\n        # 2. Chuẩn bị dữ liệu\n        # Tách features và target\n        columns_to_drop = ['total_score', 'label', 'label_encoded']\n        X_train = train_df.drop(columns=columns_to_drop, axis=1).copy()\n        X_test = test_df.drop(columns=columns_to_drop, axis=1).copy()\n\n        # Lấy nhãn (target) từ cột 'label_encoded'\n        y_train = train_df.pop(\"label_encoded\").copy()\n        y_test = test_df.pop(\"label_encoded\").copy()\n\n        # Loại bỏ cột 'user_id'\n        if 'user_id' in X_train.columns:\n            X_train.drop('user_id', axis=1, inplace=True)\n        if 'user_id' in X_test.columns:\n            X_test.drop('user_id', axis=1, inplace=True)\n\n        # Xử lý NaN trong cột 'school'\n        if 'school' in X_train.columns:\n            X_train['school'].fillna('Unknown', inplace=True)\n        if 'school' in X_test.columns:\n            X_test['school'].fillna('Unknown', inplace=True)\n\n        # Frequency Encoding cho 'school'\n        if 'school' in X_train.columns:\n            school_freq = X_train['school'].value_counts(normalize=True)\n            X_train['school_encoded'] = X_train['school'].map(school_freq)\n            X_test['school_encoded'] = X_test['school'].map(school_freq).fillna(0)\n            X_train.drop('school', axis=1, inplace=True)\n            X_test.drop('school', axis=1, inplace=True)\n\n        # Frequency Encoding cho 'course_id'\n        if 'course_id' in X_train.columns:\n            course_freq = X_train['course_id'].value_counts(normalize=True)\n            X_train['course_id_encoded'] = X_train['course_id'].map(course_freq)\n            X_test['course_id_encoded'] = X_test['course_id'].map(course_freq).fillna(0)\n            X_train.drop('course_id', axis=1, inplace=True)\n            X_test.drop('course_id', axis=1, inplace=True)\n\n        # Xác định các cột số để chuẩn hóa\n        numerical_cols_train = X_train.select_dtypes(include=['number']).columns\n\n        # Chuẩn hóa features số\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train[numerical_cols_train])\n        X_test_scaled = scaler.transform(X_test[numerical_cols_train])\n\n        # Huấn luyện mô hình SVM\n        model = SVC(kernel='rbf', C=10, gamma=0.01, class_weight='balanced', random_state=42, probability=True)\n        model.fit(X_train_scaled, y_train)\n\n        # Dự đoán trên tập kiểm tra\n        y_pred = model.predict(X_test_scaled)\n        y_pred_proba = model.predict_proba(X_test_scaled)\n\n        # Đánh giá mô hình\n        accuracy = accuracy_score(y_test, y_pred)\n        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n\n        print(f'Accuracy: {accuracy:.4f}')\n        print(f'Precision: {precision:.4f}')\n        print(f'Recall: {recall:.4f}')\n        print(f'F1-score: {f1:.4f}')\n        print('\\nClassification Report:')\n        print(classification_report(y_test, y_pred, zero_division=0))\n\n        # Vẽ confusion matrix\n        cm = confusion_matrix(y_test, y_pred)\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                    xticklabels=model.classes_, yticklabels=model.classes_)\n        plt.xlabel('Predicted Label')\n        plt.ylabel('True Label')\n        plt.title(f'Confusion Matrix - {phase_name}')\n        plt.show()\n\n        # Tính ROC AUC (nếu phù hợp cho đa lớp)\n        try:\n            roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n            print(f'ROC AUC: {roc_auc:.4f}')\n            all_results[phase_name] = {\n                'accuracy': accuracy,\n                'precision': precision,\n                'recall': recall,\n                'f1-score': f1,\n                'roc_auc': roc_auc,\n                'confusion_matrix': cm,\n                'classification_report': classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n            }\n        except ValueError as e:\n            print(f\"Không thể tính ROC AUC: {e}\")\n            all_results[phase_name] = {\n                'accuracy': accuracy,\n                'precision': precision,\n                'recall': recall,\n                'f1-score': f1,\n                'confusion_matrix': cm,\n                'classification_report': classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n            }\n\n        # Lưu kết quả\n        results_df = X_test.copy()\n        results_df['True_Label'] = y_test\n        results_df['Predicted_Label'] = y_pred\n        results_filename = f\"test_results_standard_accuracy_{phase_name}.csv\"\n        results_df.to_csv(results_filename, index=False)\n        print(f\"Results saved as {results_filename}\")\n\n    \n        # Calibrate model\n        calibrate_train_model = calibrate_model(model, X_train_scaled, y_train)\n        y_calibrate_pred = calibrate_train_model.predict(X_test_scaled)\n    \n        results_df = X_test.copy()\n        results_df['True_Label'] = y_test\n        results_df['Predicted_Label'] = y_calibrate_pred\n        results_filename = f\"test_results_calibrate_standard_accuracy_{phase_name}.csv\"\n        results_df.to_csv(results_filename, index=False)\n        print(f\"Results saved as {results_filename}\")\n\n    \n        # Feature selection và SHAP (chỉ với phase 1-2)\n        if i <= 2:\n            print(f\"\\n-------------Permutation Importance for phase: {phase_name}---------\")\n            perm_importance = plot_permutation_importance(model, X_train, X_train_scaled, y_train)\n            print(perm_importance)\n            \n            # print(f\"\\n-----------SHAP Feature Importance for phase: {phase_name}------------\")\n            # shap_values = plot_shap_importance_svm(model, X_train, X_train_scaled)\n    \n            print(f\"\\n-----Lasso Feature Selection for phase: {phase_name}----------\")\n            lasso_importance = lasso_feature_selection(X_train, X_train_scaled, y_train)\n            print(lasso_importance)\n    \n            # print(f\"\\n----------Boruta Trick: {phase_name}----------\")\n            # important_features = boruta_trick(model,X_train,  X_train_scaled, y_train)\n            # print(important_features)\n            # if i==1:\n            #     return all_results\n\n    return all_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:39:44.792258Z","iopub.execute_input":"2025-05-05T16:39:44.792549Z","iopub.status.idle":"2025-05-05T16:39:44.813166Z","shell.execute_reply.started":"2025-05-05T16:39:44.792528Z","shell.execute_reply":"2025-05-05T16:39:44.812423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    # Đọc dữ liệu từ các file CSV\n    final_phase1_train_df = pd.read_csv(\"/kaggle/input/final-data/phase1/user_train_phase_1_train.csv\")\n    final_phase2_train_df = pd.read_csv(\"/kaggle/input/final-data/phase2/user_train_phase_2_train.csv\")\n    final_phase3_train_df = pd.read_csv(\"/kaggle/input/final-data/phase3/user_train_phase_3_train.csv\")\n    final_phase4_train_df = pd.read_csv(\"/kaggle/input/final-data/phase4/user_train_phase_4_train.csv\")\n\n    final_phase1_test_df = pd.read_csv(\"/kaggle/input/final-data/phase1/user_train_phase_1_test.csv\")\n    final_phase2_test_df = pd.read_csv(\"/kaggle/input/final-data/phase2/user_train_phase_2_test.csv\")\n    final_phase3_test_df = pd.read_csv(\"/kaggle/input/final-data/phase3/user_train_phase_3_test.csv\")\n    final_phase4_test_df = pd.read_csv(\"/kaggle/input/final-data/phase4/user_train_phase_4_test.csv\")\n\n    phases_data = [\n        (final_phase1_train_df, final_phase1_test_df),\n        (final_phase2_train_df, final_phase2_test_df),\n        (final_phase3_train_df, final_phase3_test_df),\n        (final_phase4_train_df, final_phase4_test_df)\n    ]\n\n    results = train_evaluate_svm_per_phase_v2(phases_data)\n\n    # In kết quả tổng hợp (tùy chọn)\n    print(\"\\n--- Summary of Results ---\")\n    for phase, metrics in results.items():\n        print(f\"\\n{phase}:\")\n        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"  Precision: {metrics['precision']:.4f}\")\n        print(f\"  Recall: {metrics['recall']:.4f}\")\n        print(f\"  F1-score: {metrics['f1-score']:.4f}\")\n        if 'roc_auc' in metrics:\n            print(f\"  ROC AUC: {metrics['roc_auc']:.4f}\")\n        print(f\"  Confusion Matrix:\\n{metrics['confusion_matrix']}\")\n        print(f\"  Classification Report:\\n{metrics['classification_report']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:39:44.813957Z","iopub.execute_input":"2025-05-05T16:39:44.814210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}