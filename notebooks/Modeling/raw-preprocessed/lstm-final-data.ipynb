{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":241291567,"sourceType":"kernelVersion"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T16:57:47.470565Z","iopub.execute_input":"2025-05-29T16:57:47.471191Z","iopub.status.idle":"2025-05-29T16:57:47.509084Z","shell.execute_reply.started":"2025-05-29T16:57:47.471161Z","shell.execute_reply":"2025-05-29T16:57:47.507964Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/final-data/__results__.html\n/kaggle/input/final-data/__notebook__.ipynb\n/kaggle/input/final-data/__output__.json\n/kaggle/input/final-data/custom.css\n/kaggle/input/final-data/phase2/user_train_phase_2_test.csv\n/kaggle/input/final-data/phase2/user_train_phase_2.csv\n/kaggle/input/final-data/phase2/user_train_phase_2_train.csv\n/kaggle/input/final-data/__results___files/__results___144_0.png\n/kaggle/input/final-data/__results___files/__results___76_0.png\n/kaggle/input/final-data/__results___files/__results___149_0.png\n/kaggle/input/final-data/__results___files/__results___31_0.png\n/kaggle/input/final-data/__results___files/__results___66_0.png\n/kaggle/input/final-data/__results___files/__results___59_0.png\n/kaggle/input/final-data/__results___files/__results___69_0.png\n/kaggle/input/final-data/__results___files/__results___27_0.png\n/kaggle/input/final-data/__results___files/__results___81_0.png\n/kaggle/input/final-data/__results___files/__results___71_0.png\n/kaggle/input/final-data/__results___files/__results___182_0.png\n/kaggle/input/final-data/__results___files/__results___57_0.png\n/kaggle/input/final-data/__results___files/__results___77_0.png\n/kaggle/input/final-data/__results___files/__results___62_0.png\n/kaggle/input/final-data/__results___files/__results___177_0.png\n/kaggle/input/final-data/__results___files/__results___117_0.png\n/kaggle/input/final-data/__results___files/__results___73_0.png\n/kaggle/input/final-data/__results___files/__results___112_0.png\n/kaggle/input/final-data/phase1/user_train_phase_1_test.csv\n/kaggle/input/final-data/phase1/user_train_phase_1_train.csv\n/kaggle/input/final-data/phase1/user_train_phase_1.csv\n/kaggle/input/final-data/phase4/user_train_phase_4_test.csv\n/kaggle/input/final-data/phase4/user_train_phase_4.csv\n/kaggle/input/final-data/phase4/user_train_phase_4_train.csv\n/kaggle/input/final-data/phase3/user_train_phase_3_test.csv\n/kaggle/input/final-data/phase3/user_train_phase_3_train.csv\n/kaggle/input/final-data/phase3/user_train_phase_3.csv\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import (f1_score, precision_score, recall_score, accuracy_score, \n                             roc_auc_score, roc_curve, confusion_matrix)\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.calibration import CalibratedClassifierCV  \nfrom eli5.sklearn import PermutationImportance  \n\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.metrics import (\n    f1_score, precision_score, recall_score, accuracy_score,\n    roc_auc_score, confusion_matrix, roc_curve\n)\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom catboost import CatBoostClassifier\n\nfrom tensorflow.keras.utils import to_categorical\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T16:57:47.511938Z","iopub.execute_input":"2025-05-29T16:57:47.512284Z","iopub.status.idle":"2025-05-29T16:57:47.525373Z","shell.execute_reply.started":"2025-05-29T16:57:47.512261Z","shell.execute_reply":"2025-05-29T16:57:47.524397Z"}},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":"# 1. Load dữ liệu","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow.keras.utils import to_categorical\n\n\ndef load_data(phase, timesteps, one_hot=True):\n    \"\"\"\n    Đọc dữ liệu, tiền xử lý và reshape cho LSTM.\n    Trả về X_train, y_train, X_test, y_test với X có shape (n_samples, timesteps, n_features).\n    Không thực hiện scaling.\n    \"\"\"\n    # Định dạng lại tên phase để khớp file\n    phase_ = phase[:-1] + '_' + phase[-1]\n    train_fp = f\"/kaggle/input/final-data/{phase}/user_train_{phase_}_train.csv\"\n    test_fp  = f\"/kaggle/input/final-data/{phase}/user_train_{phase_}_test.csv\"\n\n    # Đọc dữ liệu\n    df_train = pd.read_csv(train_fp)\n    df_test  = pd.read_csv(test_fp)\n\n    # Mã hoá school thành tần suất\n    freq_map = df_train['school'].value_counts().to_dict()\n    df_train['school'] = df_train['school'].map(freq_map)\n    df_test ['school'] = df_test ['school'].map(freq_map)\n\n    # Fill missing\n    df_train = df_train.fillna(0)\n    df_test  = df_test.fillna(0)\n\n    # Tách nhãn\n    y_train = df_train['label_encoded'].values\n    y_test  = df_test ['label_encoded'].values\n\n    # Loại bỏ các cột không cần thiết\n    drop_cols = ['course_id', 'user_id', 'total_score', 'label', 'label_encoded']\n    X_train_df = df_train.drop(columns=drop_cols)\n    X_test_df  = df_test .drop(columns=drop_cols)\n\n    # Chuyển DataFrame thành mảng numpy\n    X_train = X_train_df.values\n    X_test  = X_test_df.values\n\n    # Cắt cho chia hết cho timesteps\n    n_train = (X_train.shape[0] // timesteps) * timesteps\n    n_test  = (X_test.shape[0]  // timesteps) * timesteps\n    X_train = X_train[:n_train]\n    y_train = y_train[:n_train]\n    X_test  = X_test[:n_test]\n    y_test  = y_test[:n_test]\n\n\n    if one_hot:\n        num_classes = len(np.unique(y_train))\n        if num_classes > 2:\n            y_train = to_categorical(y_train, num_classes=num_classes)\n            y_test  = to_categorical(y_test,  num_classes=num_classes)\n        # Với bài toán nhị phân thì có thể để nguyên, hoặc vẫn dùng one-hot nếu muốn\n    # Reshape thành (n_samples, timesteps, n_features)\n    # n_features = X_train.shape[1]\n    # X_train = X_train.reshape(-1, timesteps, n_features)\n    # X_test  = X_test.reshape(-1, timesteps, n_features)\n\n    return X_train, y_train, X_test, y_test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T16:57:47.527518Z","iopub.execute_input":"2025-05-29T16:57:47.527805Z","iopub.status.idle":"2025-05-29T16:57:47.562385Z","shell.execute_reply.started":"2025-05-29T16:57:47.527783Z","shell.execute_reply":"2025-05-29T16:57:47.561338Z"}},"outputs":[],"execution_count":81},{"cell_type":"markdown","source":"# 2. Đánh giá dữ liệu","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, X_train, y_train, X_test, y_test, class_names=None):\n    \"\"\"\n    Đánh giá model trên train và test, vẽ confusion matrix và ROC.\n    Trả về train_results và test_results giống cấu trúc ban đầu.\n    \"\"\"\n    # Xác định classes và tên lớp\n    classes = np.unique(np.concatenate([y_train, y_test]))\n    if class_names is None:\n        class_names = [str(c) for c in classes]\n\n    # Hàm lấy xác suất dự đoán\n    def get_proba(X):\n        proba = model.predict(X)\n        if proba.ndim == 1:  # binary case\n            proba = np.vstack([1 - proba, proba]).T\n        return proba\n\n    # Hàm đánh giá từng split\n    def eval_split(X, y, name):\n        y_proba = get_proba(X)\n        y_pred = np.argmax(y_proba, axis=1)\n\n        f1 = f1_score(y, y_pred, average=None, labels=classes)\n        prec = precision_score(y, y_pred, average=None, labels=classes)\n        rec  = recall_score(y, y_pred, average=None, labels=classes)\n        acc  = accuracy_score(y, y_pred)\n\n        y_bin = label_binarize(y, classes=classes)\n        auc = roc_auc_score(y_bin, y_proba, average='macro', multi_class='ovr')\n\n        print(f\"\\n--- {name} Set ---\")\n        for idx, cname in enumerate(class_names):\n            print(f\"{cname}: Prec={prec[idx]:.3f}, Rec={rec[idx]:.3f}, F1={f1[idx]:.3f}\")\n        print(f\"Accuracy: {acc:.3f}, AUC: {auc:.3f}\")\n\n        return {\n            'f1': f1,\n            'precision': prec,\n            'recall': rec,\n            'accuracy': acc,\n            'auc': auc,\n            'y_true': y,\n            'y_pred': y_pred,\n            'y_proba': y_proba\n        }\n\n    # Đánh giá train và test\n    train_results = eval_split(X_train, y_train, 'Train')\n    test_results  = eval_split(X_test,  y_test,  'Test')\n\n    # Confusion matrix cho Test\n    cm = confusion_matrix(test_results['y_true'], test_results['y_pred'], labels=classes)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix (Test)')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.tight_layout()\n    plt.show()\n\n    # ROC curves cho mỗi lớp (Test)\n    y_te_bin = label_binarize(test_results['y_true'], classes=classes)\n    plt.figure(figsize=(8, 6))\n    for idx, cname in enumerate(class_names):\n        fpr, tpr, _ = roc_curve(y_te_bin[:, idx], test_results['y_proba'][:, idx])\n        auc_score = roc_auc_score(y_te_bin[:, idx], test_results['y_proba'][:, idx])\n        plt.plot(fpr, tpr, label=f\"{cname} (AUC={auc_score:.2f})\")\n    plt.plot([0, 1], [0, 1], 'k--', label='Chance')\n    plt.title('ROC Curves (Test)')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend(loc='best')\n    plt.tight_layout()\n    plt.show()\n\n    return {\n        'train': train_results,\n        'test': test_results\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T16:57:47.564471Z","iopub.execute_input":"2025-05-29T16:57:47.564767Z","iopub.status.idle":"2025-05-29T16:57:47.592461Z","shell.execute_reply.started":"2025-05-29T16:57:47.564746Z","shell.execute_reply":"2025-05-29T16:57:47.591066Z"}},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":"# 3. Scale dữ liệu","metadata":{}},{"cell_type":"code","source":"# Áp dụng các kỹ thuật Scaling\ndef scale_data(X_train, X_test, scaler_type):\n    # Khởi tạo bộ scaler phù hợp với loại được chọn\n    if scaler_type == \"minmax\":\n        scaler = MinMaxScaler()\n    elif scaler_type == \"standard\":\n        scaler = StandardScaler()\n    elif scaler_type == \"robust\":\n        scaler = RobustScaler()\n    elif scaler_type == \"log\":\n        # Thực hiện log scaling, cộng thêm một giá trị nhỏ để tránh log(0)\n        if (X_train <= 0).any():\n            X_train = X_train + 1 - X_train.min()  # Điều chỉnh để tất cả giá trị > 0\n        X_train = np.log(X_train)  # Áp dụng log cho dữ liệu\n        X_test = np.log(X_test + 1 - X_test.min())  # Áp dụng log cho dữ liệu test\n        return X_train, X_test\n    else:\n        raise ValueError(\"Scaler type not recognized\")\n    \n    # Fit bộ scaler trên dữ liệu huấn luyện và transform dữ liệu huấn luyện và kiểm tra\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)  # Transform dữ liệu test bằng scaler đã fit trên train\n    \n    return X_train_scaled, X_test_scaled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T16:57:47.594595Z","iopub.execute_input":"2025-05-29T16:57:47.594990Z","iopub.status.idle":"2025-05-29T16:57:47.620621Z","shell.execute_reply.started":"2025-05-29T16:57:47.594938Z","shell.execute_reply":"2025-05-29T16:57:47.619463Z"}},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":"# 4. Permutation importance","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.inspection import permutation_importance\n\ndef plot_permutation_importance(model, X_train, y_train, feature_names=None, use='last'):\n    \"\"\"\n    Tính và vẽ permutation importance với model và X_train (2D hoặc 3D).\n    - Nếu X_train 3D: lấy bước thời gian cuối cùng (hoặc mean) để chuyển về 2D.\n    - model phải là model đã fit và hỗ trợ predict trên input 2D.\n    \"\"\"\n    # Nếu X_train là 3D: (n_samples, timesteps, n_features)\n    if len(X_train.shape) == 3:\n        if use == 'last':\n            X_tabular = X_train[:, -1, :]\n        elif use == 'mean':\n            X_tabular = X_train.mean(axis=1)\n        else:\n            raise ValueError(\"Giá trị use phải là 'last' hoặc 'mean'.\")\n    else:\n        X_tabular = X_train\n\n    # Nếu chưa có feature_names, tự tạo\n    if feature_names is None:\n        feature_names = [f'F{i}' for i in range(X_tabular.shape[1])]\n\n    # Nếu X_tabular không phải DataFrame, chuyển về DataFrame để dùng .columns\n    X_df = pd.DataFrame(X_tabular, columns=feature_names)\n\n    # Nếu model là keras (Sequential), phải wrap lại để trả ra dự đoán xác suất class cao nhất\n    from sklearn.base import BaseEstimator, ClassifierMixin\n    import numpy as np\n\n    class KerasWrapper(BaseEstimator, ClassifierMixin):\n        def __init__(self, model):\n            self.model = model\n        def fit(self, X, y):\n            # Không dùng fit, chỉ để tương thích API\n            return self\n        def predict(self, X):\n            proba = self.model.predict(X)\n            if proba.shape[1] == 1:\n                return (proba > 0.5).astype(int).flatten()\n            else:\n                return np.argmax(proba, axis=1)\n\n    # Nếu model là Keras (Sequential), wrap lại\n    if \"keras\" in str(type(model)).lower() or \"tensorflow\" in str(type(model)).lower():\n        model = KerasWrapper(model)\n\n    # Chạy permutation importance\n    result = permutation_importance(model, X_df, y_train, n_repeats=10, random_state=42, n_jobs=-1)\n    importance_df = pd.DataFrame({\n        \"Feature\": X_df.columns,\n        \"Importance\": result.importances_mean\n    }).sort_values(by=\"Importance\", ascending=False)\n    \n    # Vẽ biểu đồ Importance\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=\"Importance\", y=\"Feature\", data=importance_df)\n    plt.title(\"Feature Importance (Permutation Importance)\")\n    plt.xlabel(\"Importance\")\n    plt.ylabel(\"Feature\")\n    plt.tight_layout()\n    plt.show()\n    \n    return importance_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T16:57:47.621666Z","iopub.execute_input":"2025-05-29T16:57:47.621901Z","iopub.status.idle":"2025-05-29T16:57:47.644840Z","shell.execute_reply.started":"2025-05-29T16:57:47.621884Z","shell.execute_reply":"2025-05-29T16:57:47.643430Z"}},"outputs":[],"execution_count":84},{"cell_type":"markdown","source":"# Lasso","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LassoCV\n\ndef lasso_feature_selection(X_train, y_train, feature_names=None, use='last'):\n    \"\"\"\n    - X_train: array 2D hoặc 3D (nếu 3D sẽ lấy bước cuối cùng hoặc mean)\n    - y_train: label (1D)\n    - feature_names: tên cột (nếu None sẽ tự tạo)\n    - use: 'last' (bước cuối) hoặc 'mean' (trung bình trên time)\n    \"\"\"\n    # Chuyển từ 3D về 2D nếu cần\n    if len(X_train.shape) == 3:\n        if use == 'last':\n            X_tabular = X_train[:, -1, :]\n        elif use == 'mean':\n            X_tabular = X_train.mean(axis=1)\n        else:\n            raise ValueError(\"use must be 'last' hoặc 'mean'\")\n    else:\n        X_tabular = X_train\n\n    # Đảm bảo X_tabular là DataFrame\n    if feature_names is None:\n        feature_names = [f'F{i}' for i in range(X_tabular.shape[1])]\n    X_df = pd.DataFrame(X_tabular, columns=feature_names)\n\n    # Lasso Regression với Cross-Validation\n    lasso = LassoCV(cv=5, random_state=42, n_jobs=-1)\n    lasso.fit(X_df, y_train)\n\n    feature_importance = pd.DataFrame({\n        'Feature': X_df.columns,\n        'Importance': np.abs(lasso.coef_)\n    }).sort_values(by='Importance', ascending=False)\n    \n    # Vẽ biểu đồ trọng số\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n    plt.title(\"Feature Importance (Lasso Regression)\")\n    plt.xlabel(\"Importance\")\n    plt.ylabel(\"Feature\")\n    plt.tight_layout()\n    plt.show()\n    \n    selected_features = feature_importance[feature_importance['Importance'] > 0]['Feature'].tolist()\n\n    return selected_features, feature_importance\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T16:57:47.647162Z","iopub.execute_input":"2025-05-29T16:57:47.647496Z","iopub.status.idle":"2025-05-29T16:57:47.675408Z","shell.execute_reply.started":"2025-05-29T16:57:47.647473Z","shell.execute_reply":"2025-05-29T16:57:47.674299Z"}},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":"# Boruta Truck","metadata":{}},{"cell_type":"code","source":"def boruta_trick(model, X_train, y_train):\n    # Tạo một feature ngẫu nhiên (feature \"ngẫu nhiên\")\n    random_feature = np.random.rand(X_train.shape[0], 1)\n    X_train_random = np.hstack([X_train.values, random_feature])\n\n    # Huấn luyện mô hình Random Forest\n    model.fit(X_train_random, y_train)\n\n    # Tính toán importance của từng feature\n    feature_importance = pd.DataFrame({\n        'Feature': list(X_train.columns) + ['Random Feature'],\n        'Importance': model.feature_importances_\n    }).sort_values(by='Importance', ascending=False)\n\n    # Lọc các feature quan trọng hơn feature ngẫu nhiên\n    important_features = feature_importance[feature_importance['Importance'] > feature_importance['Importance'].iloc[-1]]\n    print('important feature: ', important_features)\n    # Vẽ biểu đồ importance\n    feature_importance.plot(kind='barh', x='Feature', y='Importance', legend=False, figsize=(8, 6))\n    plt.title(\"Boruta Feature Importance (Trick)\")\n    plt.xlabel(\"Importance\")\n    plt.ylabel(\"Feature\")\n    plt.show()\n\n    return important_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T16:57:47.676556Z","iopub.execute_input":"2025-05-29T16:57:47.676800Z","iopub.status.idle":"2025-05-29T16:57:47.702448Z","shell.execute_reply.started":"2025-05-29T16:57:47.676783Z","shell.execute_reply":"2025-05-29T16:57:47.701214Z"}},"outputs":[],"execution_count":86},{"cell_type":"markdown","source":"# Calibration","metadata":{}},{"cell_type":"code","source":"def calibrate_model(model, X_train, y_train, method='sigmoid'):\n    \"\"\"\n    Áp dụng Calibration cho mô hình phân loại (hỗ trợ CatBoost và các mô hình khác).\n    \n    Parameters:\n    - model: Mô hình đã huấn luyện\n    - X_train: Dữ liệu huấn luyện\n    - y_train: Nhãn huấn luyện\n    - method: Phương pháp calibration ('sigmoid' hoặc 'isotonic')\n    \n    Returns:\n    - calibrated_model: Mô hình đã được calibrate\n    \"\"\"\n    # Nếu là CatBoost, thông báo nhẹ\n    if isinstance(model, CatBoostClassifier):\n        print(\"[Warning] CatBoostClassifier đã hỗ trợ predict_proba tốt. Calibration có thể không cần thiết.\")\n    \n    # Calibration\n    calibrated_model = CalibratedClassifierCV(base_estimator=model, method=method, cv=5)\n    calibrated_model.fit(X_train, y_train)\n    \n    return calibrated_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1.Standard","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\n\n\ndef build_lstm_model(input_shape, units=64, dropout_rate=0.2, num_classes=1):\n    model = Sequential()\n    model.add(LSTM(units, input_shape=input_shape, return_sequences=False))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(num_classes, activation='sigmoid' if num_classes == 1 else 'softmax'))\n    model.compile(loss='binary_crossentropy' if num_classes == 1 else 'categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\ndef process_phase(phase, scale='standard', metric='accuracy',timesteps=1):\n    # Đọc dữ liệu\n    X_train, y_train, X_test, y_test = load_data(phase, timesteps,one_hot=False)\n\n    # if num_classes > 2:\n    #     y_train_cat = to_categorical(y_train, num_classes=num_classes)\n    # else:\n    #     y_train_cat = y_train\n    # Scale dữ liệu\n    X_train_scaled, X_test_scaled = scale_data(X_train, X_test, scaler_type=scale)\n    \n    # Đảm bảo đúng shape cho LSTM: (samples, timesteps, features)\n    if len(X_train_scaled.shape) == 2:\n        # reshape nếu đầu vào chưa đủ 3 chiều\n        X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], timesteps, X_train_scaled.shape[1]))\n        X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], timesteps, X_test_scaled.shape[1]))\n\n    input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n    num_classes = 1 if len(np.unique(y_train)) == 2 else len(np.unique(y_train))\n\n    # Thiết lập StratifiedKFold\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    best_score = -np.inf\n    best_params = {}\n    best_model = None\n\n    # Grid search thủ công (do Keras không hợp với RandomizedSearchCV)\n    for units in [32, 64]:\n        for dropout_rate in [0.1, 0.2]:\n            fold_scores = []\n            for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n                X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n                y_tr, y_val = y_train[train_idx], y_train[val_idx]\n            \n                if num_classes > 2:\n                    y_tr_cat = to_categorical(y_tr, num_classes=num_classes)\n                    y_val_cat = to_categorical(y_val, num_classes=num_classes)\n                else:\n                    y_tr_cat = y_tr\n                    y_val_cat = y_val\n            \n                model = build_lstm_model(input_shape, units=units, dropout_rate=dropout_rate, num_classes=num_classes)\n                es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n                model.fit(X_tr, y_tr_cat, validation_data=(X_val, y_val_cat), epochs=20, batch_size=64, callbacks=[es], verbose=0)\n                score = model.evaluate(X_val, y_val_cat, verbose=0)[1]  # accuracy\n                fold_scores.append(score)\n            avg_score = np.mean(fold_scores)\n            if avg_score > best_score:\n                best_score = avg_score\n                best_params = {'units': units, 'dropout_rate': dropout_rate}\n                best_model = build_lstm_model(input_shape, units=units, dropout_rate=dropout_rate, num_classes=num_classes)\n                if num_classes > 2:\n                    y_train_cat = to_categorical(y_train, num_classes=num_classes)\n                else:\n                    y_train_cat = y_train\n                best_model.fit(X_train_scaled, y_train_cat, epochs=20, batch_size=64, verbose=0)\n\n    print(f\"\\n=== Best Parameters for phase {phase}: {best_params} ===\")\n\n    # ==== Đánh giá mô hình gốc ====\n    print(f\"\\n=== Evaluation for phase {phase} ===\")\n    \n    # Chuyển y_test về dạng số nguyên (label), nếu cần\n    if y_test.ndim > 1:\n        y_test_label = np.argmax(y_test, axis=1)\n    else:\n        y_test_label = y_test\n    \n    evaluation_results = evaluate_model(\n        best_model, X_train_scaled, y_train, X_test_scaled, y_test\n    )\n    train_results = evaluation_results['train']\n    test_results = evaluation_results['test']\n    \n    for metric_name in ['f1', 'precision', 'recall', 'accuracy', 'auc']:\n        print(f\"Train {metric_name.capitalize()}: {train_results[metric_name]} | Test {metric_name.capitalize()}: {test_results[metric_name]}\")\n    \n    # ==== Lưu kết quả dự đoán ====\n    y_proba_test = best_model.predict(X_test_scaled)\n    if y_proba_test.shape[1] == 1:\n        y_pred = (y_proba_test > 0.5).astype(int).flatten()\n    else:\n        y_pred = np.argmax(y_proba_test, axis=1)\n    \n    results_df = pd.DataFrame(X_test.reshape(X_test.shape[0], -1))\n    results_df['True_Label'] = y_test_label\n    results_df['Predicted_Label'] = y_pred\n    results_filename = f\"test_results_{scale}_{metric}_{phase}.csv\"\n    results_df.to_csv(results_filename, index=False)\n    print(f\"Test predictions saved as {results_filename}\")\n\n\n    if num_classes == 2:\n        label_names = ['A', 'B']\n    else:\n        label_names = ['A', 'B', 'C', 'D', 'E']\n    \n    print(\"\\n=== Classification Report ===\")\n    print(classification_report(y_test_label, y_pred, target_names=label_names, digits=3))\n        \n    # ==== Calibrate model ====\n    y_proba_train = best_model.predict(X_train_scaled)\n    if y_train.ndim > 1:\n        y_train_label = np.argmax(y_train, axis=1)\n    else:\n        y_train_label = y_train\n    \n    # Chọn LogisticRegression cho đúng binary/multi-class\n    if y_proba_train.shape[1] == 1:\n        calibrator = LogisticRegression()\n    else:\n        calibrator = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000)\n    calibrator.fit(y_proba_train, y_train_label)\n    \n    class CalibratedLSTMWrapper:\n        def predict(self, X):\n            proba = best_model.predict(X)\n            return calibrator.predict(proba)\n    \n    calibrate_train_model = CalibratedLSTMWrapper()\n    \n    print(f\"\\n=== Evaluation after calibration for phase {phase} ===\")\n    evaluation_results_cal = evaluate_model(\n        calibrate_train_model, X_train_scaled, y_train_label, X_test_scaled, y_test_label\n    )\n    train_results_cal = evaluation_results_cal['train']\n    test_results_cal = evaluation_results_cal['test']\n    for metric_name in ['f1', 'precision', 'recall', 'accuracy', 'auc']:\n        print(f\"Train {metric_name.capitalize()} (calibrated): {train_results_cal[metric_name]} | Test {metric_name.capitalize()} (calibrated): {test_results_cal[metric_name]}\")\n    \n    # ==== Lưu kết quả dự đoán sau calibrate ====\n    y_calibrate_pred = calibrate_train_model.predict(X_test_scaled)\n    results_df['Calibrated_Predicted_Label'] = y_calibrate_pred\n    results_calibrate_filename = f\"test_results_calibrate_{scale}_{metric}_{phase}.csv\"\n    results_df.to_csv(results_calibrate_filename, index=False)\n    print(f\"Calibrated predictions saved as {results_calibrate_filename}\")\n    \n    if len(np.unique(y_train_label)) == 2:\n        label_names = ['A', 'B']\n    else:\n        label_names = ['A', 'B', 'C', 'D', 'E']\n    \n    print(\"\\n=== Classification Report after calibration ===\")\n    print(classification_report(y_test_label, y_calibrate_pred, target_names=label_names, digits=3))\n    \n    # ==== Nếu phase 1 hoặc 2 → chạy chọn feature ====\n    if int(phase[-1]) <= 2:\n        print(f\"\\n=== Permutation Importance for phase {phase} ===\")\n        perm_importance = plot_permutation_importance(best_model, X_train_scaled, y_train)\n    \n        print(f\"\\n=== Lasso Feature Selection for phase {phase} ===\")\n        lasso_importance = lasso_feature_selection(X_train_scaled, y_train)\n        # Nếu muốn dùng Boruta bạn cần chuyển đổi model phù hợp hoặc dùng tree-based model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for phase in [\"phase1\", \"phase2\", \"phase3\", \"phase4\"]:\n    process_phase(phase, metric='f1_macro', timesteps=1)  # Chỉnh timesteps tuỳ ý\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Minmax","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\n\n\ndef build_lstm_model(input_shape, units=64, dropout_rate=0.2, num_classes=1):\n    model = Sequential()\n    model.add(LSTM(units, input_shape=input_shape, return_sequences=False))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(num_classes, activation='sigmoid' if num_classes == 1 else 'softmax'))\n    model.compile(loss='binary_crossentropy' if num_classes == 1 else 'categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\ndef process_phase(phase, scale='minmax', metric='accuracy',timesteps=1):\n    # Đọc dữ liệu\n    X_train, y_train, X_test, y_test = load_data(phase, timesteps,one_hot=False)\n\n    # if num_classes > 2:\n    #     y_train_cat = to_categorical(y_train, num_classes=num_classes)\n    # else:\n    #     y_train_cat = y_train\n    # Scale dữ liệu\n    X_train_scaled, X_test_scaled = scale_data(X_train, X_test, scaler_type=scale)\n    \n    # Đảm bảo đúng shape cho LSTM: (samples, timesteps, features)\n    if len(X_train_scaled.shape) == 2:\n        # reshape nếu đầu vào chưa đủ 3 chiều\n        X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], timesteps, X_train_scaled.shape[1]))\n        X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], timesteps, X_test_scaled.shape[1]))\n\n    input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n    num_classes = 1 if len(np.unique(y_train)) == 2 else len(np.unique(y_train))\n\n    # Thiết lập StratifiedKFold\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    best_score = -np.inf\n    best_params = {}\n    best_model = None\n\n    # Grid search thủ công (do Keras không hợp với RandomizedSearchCV)\n    for units in [32, 64]:\n        for dropout_rate in [0.1, 0.2]:\n            fold_scores = []\n            for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n                X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n                y_tr, y_val = y_train[train_idx], y_train[val_idx]\n            \n                if num_classes > 2:\n                    y_tr_cat = to_categorical(y_tr, num_classes=num_classes)\n                    y_val_cat = to_categorical(y_val, num_classes=num_classes)\n                else:\n                    y_tr_cat = y_tr\n                    y_val_cat = y_val\n            \n                model = build_lstm_model(input_shape, units=units, dropout_rate=dropout_rate, num_classes=num_classes)\n                es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n                model.fit(X_tr, y_tr_cat, validation_data=(X_val, y_val_cat), epochs=20, batch_size=64, callbacks=[es], verbose=0)\n                score = model.evaluate(X_val, y_val_cat, verbose=0)[1]  # accuracy\n                fold_scores.append(score)\n            avg_score = np.mean(fold_scores)\n            if avg_score > best_score:\n                best_score = avg_score\n                best_params = {'units': units, 'dropout_rate': dropout_rate}\n                best_model = build_lstm_model(input_shape, units=units, dropout_rate=dropout_rate, num_classes=num_classes)\n                if num_classes > 2:\n                    y_train_cat = to_categorical(y_train, num_classes=num_classes)\n                else:\n                    y_train_cat = y_train\n                best_model.fit(X_train_scaled, y_train_cat, epochs=20, batch_size=64, verbose=0)\n\n    print(f\"\\n=== Best Parameters for phase {phase}: {best_params} ===\")\n\n    # ==== Đánh giá mô hình gốc ====\n    print(f\"\\n=== Evaluation for phase {phase} ===\")\n    \n    # Chuyển y_test về dạng số nguyên (label), nếu cần\n    if y_test.ndim > 1:\n        y_test_label = np.argmax(y_test, axis=1)\n    else:\n        y_test_label = y_test\n    \n    evaluation_results = evaluate_model(\n        best_model, X_train_scaled, y_train, X_test_scaled, y_test\n    )\n    train_results = evaluation_results['train']\n    test_results = evaluation_results['test']\n    \n    for metric_name in ['f1', 'precision', 'recall', 'accuracy', 'auc']:\n        print(f\"Train {metric_name.capitalize()}: {train_results[metric_name]} | Test {metric_name.capitalize()}: {test_results[metric_name]}\")\n    \n    # ==== Lưu kết quả dự đoán ====\n    y_proba_test = best_model.predict(X_test_scaled)\n    if y_proba_test.shape[1] == 1:\n        y_pred = (y_proba_test > 0.5).astype(int).flatten()\n    else:\n        y_pred = np.argmax(y_proba_test, axis=1)\n    \n    results_df = pd.DataFrame(X_test.reshape(X_test.shape[0], -1))\n    results_df['True_Label'] = y_test_label\n    results_df['Predicted_Label'] = y_pred\n    results_filename = f\"test_results_{scale}_{metric}_{phase}.csv\"\n    results_df.to_csv(results_filename, index=False)\n    print(f\"Test predictions saved as {results_filename}\")\n\n\n    if num_classes == 2:\n        label_names = ['A', 'B']\n    else:\n        label_names = ['A', 'B', 'C', 'D', 'E']\n    \n    print(\"\\n=== Classification Report ===\")\n    print(classification_report(y_test_label, y_pred, target_names=label_names, digits=3))\n        \n    # ==== Calibrate model ====\n    y_proba_train = best_model.predict(X_train_scaled)\n    if y_train.ndim > 1:\n        y_train_label = np.argmax(y_train, axis=1)\n    else:\n        y_train_label = y_train\n    \n    # Chọn LogisticRegression cho đúng binary/multi-class\n    if y_proba_train.shape[1] == 1:\n        calibrator = LogisticRegression()\n    else:\n        calibrator = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000)\n    calibrator.fit(y_proba_train, y_train_label)\n    \n    class CalibratedLSTMWrapper:\n        def predict(self, X):\n            proba = best_model.predict(X)\n            return calibrator.predict(proba)\n    \n    calibrate_train_model = CalibratedLSTMWrapper()\n    \n    print(f\"\\n=== Evaluation after calibration for phase {phase} ===\")\n    evaluation_results_cal = evaluate_model(\n        calibrate_train_model, X_train_scaled, y_train_label, X_test_scaled, y_test_label\n    )\n    train_results_cal = evaluation_results_cal['train']\n    test_results_cal = evaluation_results_cal['test']\n    for metric_name in ['f1', 'precision', 'recall', 'accuracy', 'auc']:\n        print(f\"Train {metric_name.capitalize()} (calibrated): {train_results_cal[metric_name]} | Test {metric_name.capitalize()} (calibrated): {test_results_cal[metric_name]}\")\n    \n    # ==== Lưu kết quả dự đoán sau calibrate ====\n    y_calibrate_pred = calibrate_train_model.predict(X_test_scaled)\n    results_df['Calibrated_Predicted_Label'] = y_calibrate_pred\n    results_calibrate_filename = f\"test_results_calibrate_{scale}_{metric}_{phase}.csv\"\n    results_df.to_csv(results_calibrate_filename, index=False)\n    print(f\"Calibrated predictions saved as {results_calibrate_filename}\")\n    \n    if len(np.unique(y_train_label)) == 2:\n        label_names = ['A', 'B']\n    else:\n        label_names = ['A', 'B', 'C', 'D', 'E']\n    \n    print(\"\\n=== Classification Report after calibration ===\")\n    print(classification_report(y_test_label, y_calibrate_pred, target_names=label_names, digits=3))\n    \n    # ==== Nếu phase 1 hoặc 2 → chạy chọn feature ====\n    if int(phase[-1]) <= 2:\n        print(f\"\\n=== Permutation Importance for phase {phase} ===\")\n        perm_importance = plot_permutation_importance(best_model, X_train_scaled, y_train)\n    \n        print(f\"\\n=== Lasso Feature Selection for phase {phase} ===\")\n        lasso_importance = lasso_feature_selection(X_train_scaled, y_train)\n        # Nếu muốn dùng Boruta bạn cần chuyển đổi model phù hợp hoặc dùng tree-based model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for phase in [\"phase1\", \"phase2\", \"phase3\", \"phase4\"]:\n    process_phase(phase, metric='f1_macro', timesteps=1)  # Chỉnh timesteps tuỳ ý\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Robust","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\n\n\ndef build_lstm_model(input_shape, units=64, dropout_rate=0.2, num_classes=1):\n    model = Sequential()\n    model.add(LSTM(units, input_shape=input_shape, return_sequences=False))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(num_classes, activation='sigmoid' if num_classes == 1 else 'softmax'))\n    model.compile(loss='binary_crossentropy' if num_classes == 1 else 'categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\ndef process_phase(phase, scale='robust', metric='accuracy',timesteps=1):\n    # Đọc dữ liệu\n    X_train, y_train, X_test, y_test = load_data(phase, timesteps,one_hot=False)\n\n    # if num_classes > 2:\n    #     y_train_cat = to_categorical(y_train, num_classes=num_classes)\n    # else:\n    #     y_train_cat = y_train\n    # Scale dữ liệu\n    X_train_scaled, X_test_scaled = scale_data(X_train, X_test, scaler_type=scale)\n    \n    # Đảm bảo đúng shape cho LSTM: (samples, timesteps, features)\n    if len(X_train_scaled.shape) == 2:\n        # reshape nếu đầu vào chưa đủ 3 chiều\n        X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], timesteps, X_train_scaled.shape[1]))\n        X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], timesteps, X_test_scaled.shape[1]))\n\n    input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n    num_classes = 1 if len(np.unique(y_train)) == 2 else len(np.unique(y_train))\n\n    # Thiết lập StratifiedKFold\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    best_score = -np.inf\n    best_params = {}\n    best_model = None\n\n    # Grid search thủ công (do Keras không hợp với RandomizedSearchCV)\n    for units in [32, 64]:\n        for dropout_rate in [0.1, 0.2]:\n            fold_scores = []\n            for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n                X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n                y_tr, y_val = y_train[train_idx], y_train[val_idx]\n            \n                if num_classes > 2:\n                    y_tr_cat = to_categorical(y_tr, num_classes=num_classes)\n                    y_val_cat = to_categorical(y_val, num_classes=num_classes)\n                else:\n                    y_tr_cat = y_tr\n                    y_val_cat = y_val\n            \n                model = build_lstm_model(input_shape, units=units, dropout_rate=dropout_rate, num_classes=num_classes)\n                es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n                model.fit(X_tr, y_tr_cat, validation_data=(X_val, y_val_cat), epochs=20, batch_size=64, callbacks=[es], verbose=0)\n                score = model.evaluate(X_val, y_val_cat, verbose=0)[1]  # accuracy\n                fold_scores.append(score)\n            avg_score = np.mean(fold_scores)\n            if avg_score > best_score:\n                best_score = avg_score\n                best_params = {'units': units, 'dropout_rate': dropout_rate}\n                best_model = build_lstm_model(input_shape, units=units, dropout_rate=dropout_rate, num_classes=num_classes)\n                if num_classes > 2:\n                    y_train_cat = to_categorical(y_train, num_classes=num_classes)\n                else:\n                    y_train_cat = y_train\n                best_model.fit(X_train_scaled, y_train_cat, epochs=20, batch_size=64, verbose=0)\n\n    print(f\"\\n=== Best Parameters for phase {phase}: {best_params} ===\")\n\n    # ==== Đánh giá mô hình gốc ====\n    print(f\"\\n=== Evaluation for phase {phase} ===\")\n    \n    # Chuyển y_test về dạng số nguyên (label), nếu cần\n    if y_test.ndim > 1:\n        y_test_label = np.argmax(y_test, axis=1)\n    else:\n        y_test_label = y_test\n    \n    evaluation_results = evaluate_model(\n        best_model, X_train_scaled, y_train, X_test_scaled, y_test\n    )\n    train_results = evaluation_results['train']\n    test_results = evaluation_results['test']\n    \n    for metric_name in ['f1', 'precision', 'recall', 'accuracy', 'auc']:\n        print(f\"Train {metric_name.capitalize()}: {train_results[metric_name]} | Test {metric_name.capitalize()}: {test_results[metric_name]}\")\n    \n    # ==== Lưu kết quả dự đoán ====\n    y_proba_test = best_model.predict(X_test_scaled)\n    if y_proba_test.shape[1] == 1:\n        y_pred = (y_proba_test > 0.5).astype(int).flatten()\n    else:\n        y_pred = np.argmax(y_proba_test, axis=1)\n    \n    results_df = pd.DataFrame(X_test.reshape(X_test.shape[0], -1))\n    results_df['True_Label'] = y_test_label\n    results_df['Predicted_Label'] = y_pred\n    results_filename = f\"test_results_{scale}_{metric}_{phase}.csv\"\n    results_df.to_csv(results_filename, index=False)\n    print(f\"Test predictions saved as {results_filename}\")\n\n\n    if num_classes == 2:\n        label_names = ['A', 'B']\n    else:\n        label_names = ['A', 'B', 'C', 'D', 'E']\n    \n    print(\"\\n=== Classification Report ===\")\n    print(classification_report(y_test_label, y_pred, target_names=label_names, digits=3))\n        \n    # ==== Calibrate model ====\n    y_proba_train = best_model.predict(X_train_scaled)\n    if y_train.ndim > 1:\n        y_train_label = np.argmax(y_train, axis=1)\n    else:\n        y_train_label = y_train\n    \n    # Chọn LogisticRegression cho đúng binary/multi-class\n    if y_proba_train.shape[1] == 1:\n        calibrator = LogisticRegression()\n    else:\n        calibrator = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000)\n    calibrator.fit(y_proba_train, y_train_label)\n    \n    class CalibratedLSTMWrapper:\n        def predict(self, X):\n            proba = best_model.predict(X)\n            return calibrator.predict(proba)\n    \n    calibrate_train_model = CalibratedLSTMWrapper()\n    \n    print(f\"\\n=== Evaluation after calibration for phase {phase} ===\")\n    evaluation_results_cal = evaluate_model(\n        calibrate_train_model, X_train_scaled, y_train_label, X_test_scaled, y_test_label\n    )\n    train_results_cal = evaluation_results_cal['train']\n    test_results_cal = evaluation_results_cal['test']\n    for metric_name in ['f1', 'precision', 'recall', 'accuracy', 'auc']:\n        print(f\"Train {metric_name.capitalize()} (calibrated): {train_results_cal[metric_name]} | Test {metric_name.capitalize()} (calibrated): {test_results_cal[metric_name]}\")\n    \n    # ==== Lưu kết quả dự đoán sau calibrate ====\n    y_calibrate_pred = calibrate_train_model.predict(X_test_scaled)\n    results_df['Calibrated_Predicted_Label'] = y_calibrate_pred\n    results_calibrate_filename = f\"test_results_calibrate_{scale}_{metric}_{phase}.csv\"\n    results_df.to_csv(results_calibrate_filename, index=False)\n    print(f\"Calibrated predictions saved as {results_calibrate_filename}\")\n    \n    if len(np.unique(y_train_label)) == 2:\n        label_names = ['A', 'B']\n    else:\n        label_names = ['A', 'B', 'C', 'D', 'E']\n    \n    print(\"\\n=== Classification Report after calibration ===\")\n    print(classification_report(y_test_label, y_calibrate_pred, target_names=label_names, digits=3))\n    \n    # ==== Nếu phase 1 hoặc 2 → chạy chọn feature ====\n    if int(phase[-1]) <= 2:\n        print(f\"\\n=== Permutation Importance for phase {phase} ===\")\n        perm_importance = plot_permutation_importance(best_model, X_train_scaled, y_train)\n    \n        print(f\"\\n=== Lasso Feature Selection for phase {phase} ===\")\n        lasso_importance = lasso_feature_selection(X_train_scaled, y_train)\n        # Nếu muốn dùng Boruta bạn cần chuyển đổi model phù hợp hoặc dùng tree-based model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for phase in [\"phase1\", \"phase2\", \"phase3\", \"phase4\"]:\n    process_phase(phase, metric='f1_macro', timesteps=1)  # Chỉnh timesteps tuỳ ý\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Log","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\n\n\ndef build_lstm_model(input_shape, units=64, dropout_rate=0.2, num_classes=1):\n    model = Sequential()\n    model.add(LSTM(units, input_shape=input_shape, return_sequences=False))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(num_classes, activation='sigmoid' if num_classes == 1 else 'softmax'))\n    model.compile(loss='binary_crossentropy' if num_classes == 1 else 'categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\ndef process_phase(phase, scale='log', metric='accuracy',timesteps=1):\n    # Đọc dữ liệu\n    X_train, y_train, X_test, y_test = load_data(phase, timesteps,one_hot=False)\n\n    # if num_classes > 2:\n    #     y_train_cat = to_categorical(y_train, num_classes=num_classes)\n    # else:\n    #     y_train_cat = y_train\n    # Scale dữ liệu\n    X_train_scaled, X_test_scaled = scale_data(X_train, X_test, scaler_type=scale)\n    \n    # Đảm bảo đúng shape cho LSTM: (samples, timesteps, features)\n    if len(X_train_scaled.shape) == 2:\n        # reshape nếu đầu vào chưa đủ 3 chiều\n        X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], timesteps, X_train_scaled.shape[1]))\n        X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], timesteps, X_test_scaled.shape[1]))\n\n    input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n    num_classes = 1 if len(np.unique(y_train)) == 2 else len(np.unique(y_train))\n\n    # Thiết lập StratifiedKFold\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    best_score = -np.inf\n    best_params = {}\n    best_model = None\n\n    # Grid search thủ công (do Keras không hợp với RandomizedSearchCV)\n    for units in [32, 64]:\n        for dropout_rate in [0.1, 0.2]:\n            fold_scores = []\n            for train_idx, val_idx in skf.split(X_train_scaled, y_train):\n                X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n                y_tr, y_val = y_train[train_idx], y_train[val_idx]\n            \n                if num_classes > 2:\n                    y_tr_cat = to_categorical(y_tr, num_classes=num_classes)\n                    y_val_cat = to_categorical(y_val, num_classes=num_classes)\n                else:\n                    y_tr_cat = y_tr\n                    y_val_cat = y_val\n            \n                model = build_lstm_model(input_shape, units=units, dropout_rate=dropout_rate, num_classes=num_classes)\n                es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n                model.fit(X_tr, y_tr_cat, validation_data=(X_val, y_val_cat), epochs=20, batch_size=64, callbacks=[es], verbose=0)\n                score = model.evaluate(X_val, y_val_cat, verbose=0)[1]  # accuracy\n                fold_scores.append(score)\n            avg_score = np.mean(fold_scores)\n            if avg_score > best_score:\n                best_score = avg_score\n                best_params = {'units': units, 'dropout_rate': dropout_rate}\n                best_model = build_lstm_model(input_shape, units=units, dropout_rate=dropout_rate, num_classes=num_classes)\n                if num_classes > 2:\n                    y_train_cat = to_categorical(y_train, num_classes=num_classes)\n                else:\n                    y_train_cat = y_train\n                best_model.fit(X_train_scaled, y_train_cat, epochs=20, batch_size=64, verbose=0)\n\n    print(f\"\\n=== Best Parameters for phase {phase}: {best_params} ===\")\n\n    # ==== Đánh giá mô hình gốc ====\n    print(f\"\\n=== Evaluation for phase {phase} ===\")\n    \n    # Chuyển y_test về dạng số nguyên (label), nếu cần\n    if y_test.ndim > 1:\n        y_test_label = np.argmax(y_test, axis=1)\n    else:\n        y_test_label = y_test\n    \n    evaluation_results = evaluate_model(\n        best_model, X_train_scaled, y_train, X_test_scaled, y_test\n    )\n    train_results = evaluation_results['train']\n    test_results = evaluation_results['test']\n    \n    for metric_name in ['f1', 'precision', 'recall', 'accuracy', 'auc']:\n        print(f\"Train {metric_name.capitalize()}: {train_results[metric_name]} | Test {metric_name.capitalize()}: {test_results[metric_name]}\")\n    \n    # ==== Lưu kết quả dự đoán ====\n    y_proba_test = best_model.predict(X_test_scaled)\n    if y_proba_test.shape[1] == 1:\n        y_pred = (y_proba_test > 0.5).astype(int).flatten()\n    else:\n        y_pred = np.argmax(y_proba_test, axis=1)\n    \n    results_df = pd.DataFrame(X_test.reshape(X_test.shape[0], -1))\n    results_df['True_Label'] = y_test_label\n    results_df['Predicted_Label'] = y_pred\n    results_filename = f\"test_results_{scale}_{metric}_{phase}.csv\"\n    results_df.to_csv(results_filename, index=False)\n    print(f\"Test predictions saved as {results_filename}\")\n\n\n    if num_classes == 2:\n        label_names = ['A', 'B']\n    else:\n        label_names = ['A', 'B', 'C', 'D', 'E']\n    \n    print(\"\\n=== Classification Report ===\")\n    print(classification_report(y_test_label, y_pred, target_names=label_names, digits=3))\n        \n    # ==== Calibrate model ====\n    y_proba_train = best_model.predict(X_train_scaled)\n    if y_train.ndim > 1:\n        y_train_label = np.argmax(y_train, axis=1)\n    else:\n        y_train_label = y_train\n    \n    # Chọn LogisticRegression cho đúng binary/multi-class\n    if y_proba_train.shape[1] == 1:\n        calibrator = LogisticRegression()\n    else:\n        calibrator = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=1000)\n    calibrator.fit(y_proba_train, y_train_label)\n    \n    class CalibratedLSTMWrapper:\n        def predict(self, X):\n            proba = best_model.predict(X)\n            return calibrator.predict(proba)\n    \n    calibrate_train_model = CalibratedLSTMWrapper()\n    \n    print(f\"\\n=== Evaluation after calibration for phase {phase} ===\")\n    evaluation_results_cal = evaluate_model(\n        calibrate_train_model, X_train_scaled, y_train_label, X_test_scaled, y_test_label\n    )\n    train_results_cal = evaluation_results_cal['train']\n    test_results_cal = evaluation_results_cal['test']\n    for metric_name in ['f1', 'precision', 'recall', 'accuracy', 'auc']:\n        print(f\"Train {metric_name.capitalize()} (calibrated): {train_results_cal[metric_name]} | Test {metric_name.capitalize()} (calibrated): {test_results_cal[metric_name]}\")\n    \n    # ==== Lưu kết quả dự đoán sau calibrate ====\n    y_calibrate_pred = calibrate_train_model.predict(X_test_scaled)\n    results_df['Calibrated_Predicted_Label'] = y_calibrate_pred\n    results_calibrate_filename = f\"test_results_calibrate_{scale}_{metric}_{phase}.csv\"\n    results_df.to_csv(results_calibrate_filename, index=False)\n    print(f\"Calibrated predictions saved as {results_calibrate_filename}\")\n    \n    if len(np.unique(y_train_label)) == 2:\n        label_names = ['A', 'B']\n    else:\n        label_names = ['A', 'B', 'C', 'D', 'E']\n    \n    print(\"\\n=== Classification Report after calibration ===\")\n    print(classification_report(y_test_label, y_calibrate_pred, target_names=label_names, digits=3))\n    \n    # ==== Nếu phase 1 hoặc 2 → chạy chọn feature ====\n    if int(phase[-1]) <= 2:\n        print(f\"\\n=== Permutation Importance for phase {phase} ===\")\n        perm_importance = plot_permutation_importance(best_model, X_train_scaled, y_train)\n    \n        print(f\"\\n=== Lasso Feature Selection for phase {phase} ===\")\n        lasso_importance = lasso_feature_selection(X_train_scaled, y_train)\n        # Nếu muốn dùng Boruta bạn cần chuyển đổi model phù hợp hoặc dùng tree-based model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for phase in [\"phase1\", \"phase2\", \"phase3\", \"phase4\"]:\n    process_phase(phase, metric='f1_macro', timesteps=1)  # Chỉnh timesteps tuỳ ý\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}